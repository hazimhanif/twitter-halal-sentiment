{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Twitter Data using Trained Word2Seq and Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs prediction on the Twitter dataset gathered by the University of Malaya Halal research group.  \n",
    "Therefore, the data is not publicly exposed and can be made available upon further request.  \n",
    "The prediction of the Twitter data uses the trained **Word2Vec** and **Word2Seq** models.  \n",
    "The list of models available are:\n",
    "* Word2Seq Convolutional Neural Network\n",
    "* Word2Seq Long Short Term Memory\n",
    "* Word2Seq Convolutional Neural Network + Long Short Term Memory\n",
    "* Word2Seq Convolutional Neural Nwtwork + Bi-directional Recurrent Neural Network + Bi-directional Long Short Term Memory\n",
    "* Word2Vec Convolutional Neural Network\n",
    "* Word2Vec Long Short Term Memory\n",
    "* Word2Vec Convolutional Neural Network + Long Short Term Memory\n",
    "* Word2Vec Convolutional Neural Nwtwork + Bi-directional Recurrent Neural Network + Bi-directional Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.preprocessing import sequence as keras_seq\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import set_random_seed\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Set seed\n",
    "myrand=58584\n",
    "np.random.seed(myrand)\n",
    "set_random_seed(myrand)\n",
    "\n",
    "WORDS_SIZE=8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Load and prepare the collected Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105542, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data.xlsx',sheet_name='Sheet1')\n",
    "data.text = data.text.astype(str)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>hash</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>0e8df15d4bd22ee2a025e2ba244afc4e</td>\n",
       "      <td>Darah ko ni Dah kira Halal JAKIM</td>\n",
       "      <td>2014-01-06T08:46:19.000Z</td>\n",
       "      <td>Mylea_Skincare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>8b8a05bb640c3a08f13da6beefb2c458</td>\n",
       "      <td>Menurut kajian ada sesetengah pemakan rasuah o...</td>\n",
       "      <td>2014-01-06T08:45:11.000Z</td>\n",
       "      <td>Mylea_Skincare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>c6863b3517ccedc7c05b9acf9fb71d1b</td>\n",
       "      <td>we have a full range of cleansing and skincare...</td>\n",
       "      <td>2014-01-01T23:15:59.000Z</td>\n",
       "      <td>halalcosco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>5f5722fc7e900ec7074d702a8f3d3343</td>\n",
       "      <td>Inovasi skin care terkini bebas mercury dan no...</td>\n",
       "      <td>2014-01-01T04:35:54.000Z</td>\n",
       "      <td>rullynursesi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>halal_trip.json</td>\n",
       "      <td>1bb31c3e8faebddec3c158a017be21f0</td>\n",
       "      <td>Wuih suami istri ikutan open trip I thought th...</td>\n",
       "      <td>2018-04-07T08:49:15.000Z</td>\n",
       "      <td>sayannisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>halal_trip.json</td>\n",
       "      <td>0d6140e38f2aa4f4414c3080ba651e6a</td>\n",
       "      <td>Love it check it out halalexpo</td>\n",
       "      <td>2018-04-06T12:11:39.000Z</td>\n",
       "      <td>MillanUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>halal_trip.json</td>\n",
       "      <td>23cbd1544d1e4359a248ba9898efa3db</td>\n",
       "      <td>Trip hobi traveling generasi muslim milenial d...</td>\n",
       "      <td>2018-04-06T00:10:58.000Z</td>\n",
       "      <td>Irsyad_af21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>937a238c510e64d0c33110a6639412dc</td>\n",
       "      <td>Halal is a requirement not only for food and b...</td>\n",
       "      <td>2013-12-30T19:18:30.000Z</td>\n",
       "      <td>Famiza72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>b1f410a86f23cd081b78c74cae03d74c</td>\n",
       "      <td>Love my pretty purchases from latifahalalbeaut...</td>\n",
       "      <td>2013-12-30T14:08:47.000Z</td>\n",
       "      <td>BlossomAndBean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>halal_skincare.json</td>\n",
       "      <td>3730d033aaa900c211c296a40da48db2</td>\n",
       "      <td>Soyeux Skin Care adalah produk halal dan selam...</td>\n",
       "      <td>2013-12-25T03:52:14.000Z</td>\n",
       "      <td>soyeuxofficial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name                              hash  \\\n",
       "0  halal_skincare.json  0e8df15d4bd22ee2a025e2ba244afc4e   \n",
       "1  halal_skincare.json  8b8a05bb640c3a08f13da6beefb2c458   \n",
       "2  halal_skincare.json  c6863b3517ccedc7c05b9acf9fb71d1b   \n",
       "3  halal_skincare.json  5f5722fc7e900ec7074d702a8f3d3343   \n",
       "4      halal_trip.json  1bb31c3e8faebddec3c158a017be21f0   \n",
       "5      halal_trip.json  0d6140e38f2aa4f4414c3080ba651e6a   \n",
       "6      halal_trip.json  23cbd1544d1e4359a248ba9898efa3db   \n",
       "7  halal_skincare.json  937a238c510e64d0c33110a6639412dc   \n",
       "8  halal_skincare.json  b1f410a86f23cd081b78c74cae03d74c   \n",
       "9  halal_skincare.json  3730d033aaa900c211c296a40da48db2   \n",
       "\n",
       "                                                text  \\\n",
       "0                   Darah ko ni Dah kira Halal JAKIM   \n",
       "1  Menurut kajian ada sesetengah pemakan rasuah o...   \n",
       "2  we have a full range of cleansing and skincare...   \n",
       "3  Inovasi skin care terkini bebas mercury dan no...   \n",
       "4  Wuih suami istri ikutan open trip I thought th...   \n",
       "5                     Love it check it out halalexpo   \n",
       "6  Trip hobi traveling generasi muslim milenial d...   \n",
       "7  Halal is a requirement not only for food and b...   \n",
       "8  Love my pretty purchases from latifahalalbeaut...   \n",
       "9  Soyeux Skin Care adalah produk halal dan selam...   \n",
       "\n",
       "                  timestamp           user_  \n",
       "0  2014-01-06T08:46:19.000Z  Mylea_Skincare  \n",
       "1  2014-01-06T08:45:11.000Z  Mylea_Skincare  \n",
       "2  2014-01-01T23:15:59.000Z      halalcosco  \n",
       "3  2014-01-01T04:35:54.000Z    rullynursesi  \n",
       "4  2018-04-07T08:49:15.000Z       sayannisa  \n",
       "5  2018-04-06T12:11:39.000Z        MillanUS  \n",
       "6  2018-04-06T00:10:58.000Z     Irsyad_af21  \n",
       "7  2013-12-30T19:18:30.000Z        Famiza72  \n",
       "8  2013-12-30T14:08:47.000Z  BlossomAndBean  \n",
       "9  2013-12-25T03:52:14.000Z  soyeuxofficial  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load and prepare the tokkenizer for Word2Seq and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('../../../../../Master (Sentiment Analysis)/Paper/Paper 3/Datasets/eRezeki/eRezeki_(text_class)_unclean.csv',header=0,encoding='utf-8')\n",
    "mydata = mydata.loc[mydata['sentiment'] != \"neutral\"]\n",
    "mydata['sentiment'] = mydata['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "mydata1 = pd.read_csv('../../../../../Master (Sentiment Analysis)/Paper/Paper 3/Datasets/IMDB/all_random.csv',header=0,encoding='utf-8')\n",
    "mydata = mydata.append(mydata1)\n",
    "mydata = shuffle(mydata)\n",
    "\n",
    "mydata1 = pd.read_csv('../../../../../Master (Sentiment Analysis)/Paper/Paper 3/Datasets/Amazon(sports_outdoors)/Amazon_UCSD.csv',header=0,encoding='utf-8')\n",
    "mydata1['feedback'] = mydata1['feedback'].astype(str)\n",
    "mydata = mydata.append(mydata1)\n",
    "mydata = shuffle(mydata)\n",
    "\n",
    "mydata1 = pd.read_csv('../../../../../Master (Sentiment Analysis)/Paper/Paper 3/Datasets/Yelp(zhang_paper)/yelp_zhang.csv',header=0,encoding='utf-8')\n",
    "mydata1['feedback'] = mydata1['feedback'].astype(str)\n",
    "mydata = mydata.append(mydata1)\n",
    "\n",
    "del(mydata1)\n",
    "gc.collect()\n",
    "\n",
    "mydata = shuffle(mydata)\n",
    "mydata = shuffle(mydata)\n",
    "mydata = shuffle(mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create tokkenizer from full list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts(list(mydata['feedback']))\n",
    "tokenizer.num_words=WORDS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the trained models\n",
    "\n",
    "Create dictionary for different input sizes for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir('../Models/')\n",
    "input_sizes = {'word2seq_cnn':700,\n",
    "               'word2seq_cnn_birnn_bilstm':100,\n",
    "               'word2seq_cnn_lstm':500,\n",
    "               'word2seq_lstm':100,\n",
    "               'word2vec_cnn':700,\n",
    "               'word2vec_cnn_birnn_bilstm':100,\n",
    "               'word2vec_cnn_lstm':500,\n",
    "               'word2vec_lstm':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function for sequence data matrix creation from Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(input_size):\n",
    "    list_tokenized = tokenizer.texts_to_sequences(list(data.text))\n",
    "    x_data = keras_seq.pad_sequences(list_tokenized, \n",
    "                                     maxlen=input_size,\n",
    "                                     padding='post')\n",
    "    x_data = x_data.astype(np.int64)\n",
    "    return(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function for loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    mydir = '../Models/%s/%s.hdf5' % (model_name,model_name)\n",
    "    model = load_model(mydir)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function for predict the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(model,x_data):\n",
    "    sentiment = model.predict_classes(x_data)\n",
    "    sentiment = sentiment.astype(str)\n",
    "    sentiment[sentiment=='1'] = \"Positive\"\n",
    "    sentiment[sentiment=='0'] = \"Negative\"\n",
    "    probability = model.predict_proba(x_data)\n",
    "    positive_probability = probability[:,1]\n",
    "    negative_probabiltiy = probability[:,0]\n",
    "    return(sentiment, positive_probability, negative_probabiltiy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function to add new column to the excel dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(data, model_name, sentiment, positive_probability, negative_probabiltiy):\n",
    "    name_1 = '%s_sentiment' % (model_name)\n",
    "    name_2 = '%s_posProb' % (model_name)\n",
    "    name_3 = '%s_negProb' % (model_name)\n",
    "    data['name_1'] = sentiment\n",
    "    data['name_2'] = positive_probability\n",
    "    data['name_3'] = negative_probabiltiy\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Start looping to predict the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models_list:\n",
    "    x_data = create_seq(input_sizes[name])\n",
    "    model = load_model(name)\n",
    "    sentiment, positive_prob, negative_prob = predict_data(model, x_data)\n",
    "    add_columns(data, name, sentiment, positive_prob, negative_prob)\n",
    "\n",
    "data.head(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
